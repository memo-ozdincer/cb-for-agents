{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "48ebe8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/homebrew/anaconda3/lib/python3.13/site-packages (4.4.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.3.2)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (1.2.1)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.11.10)\n",
      "Requirement already satisfied: anyio in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (4.7.0)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: shellingham in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: typer-slim in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (8.1.8)\n",
      "âœ… Project structure created at ..\n",
      "âœ… Database path: ../data/db/unified.db\n",
      "âœ… All directories ready\n"
     ]
    }
   ],
   "source": [
    "!pip3 install datasets\n",
    "# ============================================================================\n",
    "# CELL 1: SETUP & ENVIRONMENT\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "from datetime import datetime\n",
    "import zipfile\n",
    "import shutil\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Dataset loading\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Create project structure\n",
    "BASE_DIR = Path(\"../\")\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "DB_DIR = DATA_DIR / \"db\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "# Create directories\n",
    "for directory in [DATA_DIR, DB_DIR, PROCESSED_DIR, \n",
    "                  DATA_DIR / \"weblinx\",\n",
    "                  DATA_DIR / \"webarena\" / \"config_files\",\n",
    "                  DATA_DIR / \"webarena\" / \"human_trajectories\",\n",
    "                  DATA_DIR / \"webarena\" / \"llm_trajectories_v2\",\n",
    "                  DATA_DIR / \"tau2\" / \"domains\",\n",
    "                  DATA_DIR / \"tau2\" / \"results\",\n",
    "                  DATA_DIR / \"tau2\" / \"user_simulator\"]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Initialize SQLite database\n",
    "DB_PATH = DB_DIR / \"unified.db\"\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "print(f\"âœ… Project structure created at {BASE_DIR}\")\n",
    "print(f\"âœ… Database path: {DB_PATH}\")\n",
    "print(f\"âœ… All directories ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e3f1b1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns: ['demo', 'turn', 'action', 'action_history', 'utterances', 'candidates', 'clean_html', 'viewport']\n",
      "First sample keys: dict_keys(['demo', 'turn', 'action', 'action_history', 'utterances', 'candidates', 'clean_html', 'viewport'])\n"
     ]
    }
   ],
   "source": [
    "# Add this right after loading weblinx_val\n",
    "print(\"Available columns:\", weblinx_val.column_names)\n",
    "print(\"First sample keys:\", weblinx_val[0].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8fa57942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Loading WebLINX dataset from HuggingFace...\n",
      "âœ… Loaded 2126 validation samples\n",
      "ğŸ” Columns found: ['demo', 'turn', 'action', 'action_history', 'utterances', 'candidates', 'clean_html', 'viewport']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f69e9d346494cf8a893760fb21c1129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e42d7e58f544985b7f24eed3ee409f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ Inserted 100 WebLINX records\n",
      "  âœ“ Inserted 200 WebLINX records\n",
      "  âœ“ Inserted 300 WebLINX records\n",
      "  âœ“ Inserted 400 WebLINX records\n",
      "  âœ“ Inserted 500 WebLINX records\n",
      "  âœ“ Inserted 600 WebLINX records\n",
      "  âœ“ Inserted 700 WebLINX records\n",
      "  âœ“ Inserted 800 WebLINX records\n",
      "  âœ“ Inserted 900 WebLINX records\n",
      "  âœ“ Inserted 1000 WebLINX records\n",
      "âœ… WebLINX data inserted into database (1000 samples)\n",
      "âœ… Sample saved to ../data/processed/weblinx_sample.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: INGEST WEBLINX DATASET (CORRECTED KEYS)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"ğŸ”„ Loading WebLINX dataset from HuggingFace...\")\n",
    "\n",
    "# Load validation and test splits\n",
    "weblinx_val = load_dataset(\"McGill-NLP/weblinx\", split=\"validation\")\n",
    "weblinx_test_iid = load_dataset(\"McGill-NLP/weblinx\", split=\"test_iid\")\n",
    "\n",
    "print(f\"âœ… Loaded {len(weblinx_val)} validation samples\")\n",
    "print(f\"ğŸ” Columns found: {weblinx_val.column_names}\")\n",
    "\n",
    "# Download templates (for preprocessing)\n",
    "template_dir = DATA_DIR / \"weblinx\" / \"templates\"\n",
    "snapshot_download(\n",
    "    \"McGill-NLP/WebLINX\",\n",
    "    repo_type=\"dataset\",\n",
    "    allow_patterns=\"templates/*\",\n",
    "    local_dir=DATA_DIR / \"weblinx\"\n",
    ")\n",
    "\n",
    "# Create SQLite table for WebLINX\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS weblinx (\n",
    "        id TEXT PRIMARY KEY,\n",
    "        demo_id TEXT,\n",
    "        turn_id INT,\n",
    "        action TEXT,\n",
    "        action_history TEXT,\n",
    "        utterances TEXT,\n",
    "        candidates TEXT,\n",
    "        clean_html TEXT,\n",
    "        viewport TEXT,\n",
    "        source TEXT DEFAULT 'weblinx'\n",
    "    )\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "# Insert WebLINX data (sample first 1000)\n",
    "weblinx_combined = weblinx_val.select(range(min(1000, len(weblinx_val))))\n",
    "\n",
    "for idx, sample in enumerate(weblinx_combined):\n",
    "    # CORRECT KEYS: 'demo' and 'turn' (not demo_id/turn_id)\n",
    "    demo_id = sample['demo']\n",
    "    turn_id = sample['turn']\n",
    "    \n",
    "    doc_id = f\"weblinx_{demo_id}_{turn_id}\"\n",
    "    \n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT OR REPLACE INTO weblinx VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", (\n",
    "        doc_id,\n",
    "        demo_id,\n",
    "        turn_id,\n",
    "        str(sample.get('action', '')),\n",
    "        str(sample.get('action_history', '')),\n",
    "        str(sample.get('utterances', '')),\n",
    "        str(sample.get('candidates', '')),\n",
    "        str(sample.get('clean_html', ''))[:5000], # Truncate HTML\n",
    "        str(sample.get('viewport', '')),\n",
    "        'weblinx'\n",
    "    ))\n",
    "    \n",
    "    if (idx + 1) % 100 == 0:\n",
    "        print(f\"  âœ“ Inserted {idx + 1} WebLINX records\")\n",
    "\n",
    "conn.commit()\n",
    "print(f\"âœ… WebLINX data inserted into database ({len(weblinx_combined)} samples)\")\n",
    "\n",
    "# Save WebLINX JSON for reference\n",
    "# Save WebLINX JSON for reference\n",
    "weblinx_json_path = PROCESSED_DIR / \"weblinx_sample.json\"\n",
    "\n",
    "# FIX: dataset[:100] returns a dict of lists. \n",
    "# We need to iterate the dataset object to get rows.\n",
    "json_safe_sample = []\n",
    "\n",
    "# Iterate over the first 100 rows explicitly\n",
    "for i in range(min(100, len(weblinx_combined))):\n",
    "    row = weblinx_combined[i] # Accessing by index yields a dict (row)\n",
    "    # Convert all values to string to be safe for JSON\n",
    "    safe_row = {k: str(v) for k, v in row.items()}\n",
    "    json_safe_sample.append(safe_row)\n",
    "\n",
    "with open(weblinx_json_path, \"w\") as f:\n",
    "    json.dump(json_safe_sample, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Sample saved to {weblinx_json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "39c3b4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found WebArena config at ../data/webarena/config_files/test.raw.json\n",
      "âœ… Loaded 812 WebArena tasks\n",
      "âœ… WebArena tasks inserted into database\n",
      "âœ… Sample saved to ../data/processed/webarena_tasks_sample.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: INGEST WEBARENA CONFIG (test.raw.json)\n",
    "# ============================================================================\n",
    "# Assumes you've already downloaded test.raw.json manually\n",
    "\n",
    "webarena_config_path = DATA_DIR / \"webarena\" / \"config_files\" / \"test.raw.json\"\n",
    "\n",
    "if webarena_config_path.exists():\n",
    "    print(f\"âœ… Found WebArena config at {webarena_config_path}\")\n",
    "    \n",
    "    with open(webarena_config_path, 'r') as f:\n",
    "        webarena_tasks = json.load(f)\n",
    "    \n",
    "    print(f\"âœ… Loaded {len(webarena_tasks)} WebArena tasks\")\n",
    "    \n",
    "    # Create table for WebArena tasks\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS webarena_tasks (\n",
    "            task_id INTEGER PRIMARY KEY,\n",
    "            sites TEXT,\n",
    "            require_login BOOLEAN,\n",
    "            start_url TEXT,\n",
    "            intent TEXT,\n",
    "            intent_template TEXT,\n",
    "            eval_types TEXT,\n",
    "            reference_answers TEXT,\n",
    "            source TEXT DEFAULT 'webarena'\n",
    "        )\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    \n",
    "    # Insert WebArena tasks\n",
    "    for task in webarena_tasks:\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT OR REPLACE INTO webarena_tasks VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\", (\n",
    "            task.get('task_id'),\n",
    "            str(task.get('sites', [])),\n",
    "            task.get('require_login', False),\n",
    "            task.get('start_url', ''),\n",
    "            task.get('intent', ''),\n",
    "            task.get('intent_template', ''),\n",
    "            str(task.get('eval', {}).get('eval_types', [])),\n",
    "            json.dumps(task.get('eval', {}).get('reference_answers', {})),\n",
    "            'webarena'\n",
    "        ))\n",
    "    \n",
    "    conn.commit()\n",
    "    print(f\"âœ… WebArena tasks inserted into database\")\n",
    "    \n",
    "    # Save sample\n",
    "    webarena_sample_path = PROCESSED_DIR / \"webarena_tasks_sample.json\"\n",
    "    with open(webarena_sample_path, \"w\") as f:\n",
    "        json.dump(webarena_tasks[:100], f, indent=2)\n",
    "    print(f\"âœ… Sample saved to {webarena_sample_path}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"âš ï¸  WebArena config not found at {webarena_config_path}\")\n",
    "    print(f\"   Expected location: {webarena_config_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2c5aea12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Processing WebArena human trajectories from ../data/webarena/human_trajectories...\n",
      "   Found 2 .trace directories\n",
      "âœ… Inserted 2 human trace records\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: PARSE WEBARENA HUMAN TRAJECTORIES (.trace format)\n",
    "# ============================================================================\n",
    "\n",
    "def parse_trace_directory(trace_dir: Path) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse .trace directory from human trajectories.\n",
    "    Structure:\n",
    "    - resources/\n",
    "    - trace.network\n",
    "    - trace.stacks\n",
    "    - trace.trace\n",
    "    \"\"\"\n",
    "    files_present = {\n",
    "        \"has_resources\": (trace_dir / \"resources\").exists(),\n",
    "        \"has_network\": (trace_dir / \"trace.network\").exists(),\n",
    "        \"has_stacks\": (trace_dir / \"trace.stacks\").exists(),\n",
    "        \"has_trace\": (trace_dir / \"trace.trace\").exists(),\n",
    "    }\n",
    "    \n",
    "    result = {\n",
    "        \"trace_dir\": trace_dir.name,\n",
    "        \"files\": files_present,\n",
    "        \"resource_count\": 0\n",
    "    }\n",
    "    \n",
    "    # Count resources\n",
    "    if files_present[\"has_resources\"]:\n",
    "        resource_files = list((trace_dir / \"resources\").glob(\"*\"))\n",
    "        result[\"resource_count\"] = len(resource_files)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Create table for WebArena human trajectories\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS webarena_human_traces (\n",
    "        trace_id TEXT PRIMARY KEY,\n",
    "        trace_dir_name TEXT,\n",
    "        has_network BOOLEAN,\n",
    "        has_stacks BOOLEAN,\n",
    "        has_trace BOOLEAN,\n",
    "        resource_count INT,\n",
    "        source TEXT DEFAULT 'webarena_human_traces'\n",
    "    )\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "human_traj_dir = DATA_DIR / \"webarena\" / \"human_trajectories\"\n",
    "\n",
    "if human_traj_dir.exists():\n",
    "    print(f\"ğŸ”„ Processing WebArena human trajectories from {human_traj_dir}...\")\n",
    "    \n",
    "    # Find all .trace directories (e.g., 4.trace, 7.trace, etc.)\n",
    "    trace_dirs = [d for d in human_traj_dir.iterdir() if d.is_dir() and d.name.endswith('.trace')]\n",
    "    \n",
    "    print(f\"   Found {len(trace_dirs)} .trace directories\")\n",
    "    \n",
    "    for trace_dir in sorted(trace_dirs):\n",
    "        try:\n",
    "            parsed = parse_trace_directory(trace_dir)\n",
    "            trace_id = f\"webarena_human_{trace_dir.name}\"\n",
    "            \n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT OR REPLACE INTO webarena_human_traces VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\", (\n",
    "                trace_id,\n",
    "                parsed['trace_dir'],\n",
    "                parsed['files']['has_network'],\n",
    "                parsed['files']['has_stacks'],\n",
    "                parsed['files']['has_trace'],\n",
    "                parsed['resource_count'],\n",
    "                'webarena_human_traces'\n",
    "            ))\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸  Error processing {trace_dir.name}: {e}\")\n",
    "    \n",
    "    conn.commit()\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM webarena_human_traces\")\n",
    "    count = cursor.fetchone()[0]\n",
    "    print(f\"âœ… Inserted {count} human trace records\")\n",
    "else:\n",
    "    print(f\"âš ï¸  Human trajectories directory not found at {human_traj_dir}\")\n",
    "    print(f\"   Expected structure: {human_traj_dir}/4.trace, 7.trace, etc.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7a2d107e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Processing WebArena LLM v2 trajectories from ../data/webarena/llm_trajectories_v2...\n",
      "   Found 1 model directories\n",
      "   Processing 919_gpt4_8k_cot: 810 tasks\n",
      "âœ… Inserted 810 LLM trajectory records\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: PARSE WEBARENA LLM TRAJECTORIES v2 (HTML render format)\n",
    "# ============================================================================\n",
    "\n",
    "def parse_merged_log(log_path: Path) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Parse merged_log.txt from LLM trajectories.\n",
    "    Extract: Intent, Result (PASS/FAIL) for each render_*.html\n",
    "    \n",
    "    Format:\n",
    "    2023-09-24 16:32:42,509 - INFO - [Intent]: What is the top-1 best-selling product in 2022\n",
    "    2023-09-24 16:33:07,065 - INFO - [Result] (FAIL) /path/to/0.json\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    with open(log_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    current_task_id = None\n",
    "    current_intent = None\n",
    "    \n",
    "    for line in lines:\n",
    "        # Extract intent\n",
    "        if \"[Intent]:\" in line:\n",
    "            match = re.search(r\"\\[Intent\\]:\\s*(.+?)(?:\\s|$)\", line)\n",
    "            if match:\n",
    "                current_intent = match.group(1).strip()\n",
    "        \n",
    "        # Extract result (PASS/FAIL)\n",
    "        if \"[Result]\" in line:\n",
    "            # Extract task ID from path (e.g., /tmp/.../0.json â†’ 0)\n",
    "            match_result = re.search(r\"\\((\\w+)\\)\", line)\n",
    "            match_task = re.search(r\"/(\\d+)\\.json\", line)\n",
    "            \n",
    "            if match_result and match_task:\n",
    "                task_id = match_task.group(1)\n",
    "                result_status = match_result.group(1)\n",
    "                \n",
    "                results[task_id] = {\n",
    "                    \"intent\": current_intent,\n",
    "                    \"result\": result_status.lower() == \"pass\"\n",
    "                }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def parse_webarena_html(html_path: Path) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse render_*.html files from WebArena LLM trajectories.\n",
    "    Extract: observations, URLs, predictions, actions\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(html_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        \n",
    "        observations = [div.get_text()[:200] for div in soup.find_all(\"div\", {\"class\": \"state_obv\"})]\n",
    "        urls = [h3.get_text() for h3 in soup.find_all(\"h3\", {\"class\": \"url\"})]\n",
    "        raw_predictions = [div.get_text()[:200] for div in soup.find_all(\"div\", {\"class\": \"raw_parsed_prediction\"})]\n",
    "        actions = [div.get_text()[:200] for div in soup.find_all(\"div\", {\"class\": \"action_object\"})]\n",
    "        \n",
    "        return {\n",
    "            \"observations\": observations,\n",
    "            \"urls\": urls,\n",
    "            \"predictions\": raw_predictions,\n",
    "            \"actions\": actions,\n",
    "            \"num_steps\": len(observations)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e), \"num_steps\": 0}\n",
    "\n",
    "# Create table for WebArena LLM trajectories\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS webarena_llm_traces (\n",
    "        trajectory_id TEXT PRIMARY KEY,\n",
    "        model TEXT,\n",
    "        config TEXT,\n",
    "        task_id INT,\n",
    "        intent TEXT,\n",
    "        passed BOOLEAN,\n",
    "        num_steps INT,\n",
    "        observations TEXT,\n",
    "        actions TEXT,\n",
    "        urls TEXT,\n",
    "        predictions TEXT,\n",
    "        source TEXT DEFAULT 'webarena_llm_traces'\n",
    "    )\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "llm_traj_v2_dir = DATA_DIR / \"webarena\" / \"llm_trajectories_v2\"\n",
    "\n",
    "if llm_traj_v2_dir.exists():\n",
    "    print(f\"ğŸ”„ Processing WebArena LLM v2 trajectories from {llm_traj_v2_dir}...\")\n",
    "    \n",
    "    # Find all unzipped model folders (e.g., v2_919_gpt4_8k_cot/)\n",
    "    model_dirs = [d for d in llm_traj_v2_dir.iterdir() if d.is_dir()]\n",
    "    \n",
    "    print(f\"   Found {len(model_dirs)} model directories\")\n",
    "    \n",
    "    for model_dir in sorted(model_dirs):\n",
    "        # Parse model name from directory\n",
    "        # Expected format: v2_919_gpt4_8k_cot or similar\n",
    "        model_name = model_dir.name\n",
    "        \n",
    "        # Parse merged_log.txt for pass/fail + intent\n",
    "        log_file = model_dir / \"merged_log.txt\"\n",
    "        if not log_file.exists():\n",
    "            print(f\"   âš ï¸  No merged_log.txt in {model_name}\")\n",
    "            continue\n",
    "        \n",
    "        log_data = parse_merged_log(log_file)\n",
    "        print(f\"   Processing {model_name}: {len(log_data)} tasks\")\n",
    "        \n",
    "        # Parse all render_*.html files\n",
    "        for html_file in sorted(model_dir.glob(\"render_*.html\")):\n",
    "            try:\n",
    "                task_id = int(html_file.stem.replace(\"render_\", \"\"))\n",
    "                parsed_html = parse_webarena_html(html_file)\n",
    "                \n",
    "                # Get intent and result from log\n",
    "                log_info = log_data.get(str(task_id), {})\n",
    "                intent = log_info.get(\"intent\", \"unknown\")\n",
    "                passed = log_info.get(\"result\", False)\n",
    "                \n",
    "                trajectory_id = f\"webarena_llm_{model_name}_task_{task_id}\"\n",
    "                \n",
    "                cursor.execute(\"\"\"\n",
    "                    INSERT OR REPLACE INTO webarena_llm_traces VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                \"\"\", (\n",
    "                    trajectory_id,\n",
    "                    model_name,\n",
    "                    \"\",  # config can be parsed from model_name if needed\n",
    "                    task_id,\n",
    "                    intent,\n",
    "                    passed,\n",
    "                    parsed_html.get(\"num_steps\", 0),\n",
    "                    json.dumps(parsed_html.get(\"observations\", [])),\n",
    "                    json.dumps(parsed_html.get(\"actions\", [])),\n",
    "                    json.dumps(parsed_html.get(\"urls\", [])),\n",
    "                    json.dumps(parsed_html.get(\"predictions\", [])),\n",
    "                    'webarena_llm_traces'\n",
    "                ))\n",
    "            except Exception as e:\n",
    "                print(f\"     âš ï¸  Error processing {html_file}: {e}\")\n",
    "        \n",
    "        conn.commit()\n",
    "    \n",
    "    cursor.execute(\"SELECT COUNT(*) FROM webarena_llm_traces\")\n",
    "    count = cursor.fetchone()[0]\n",
    "    print(f\"âœ… Inserted {count} LLM trajectory records\")\n",
    "else:\n",
    "    print(f\"âš ï¸  LLM trajectories v2 directory not found at {llm_traj_v2_dir}\")\n",
    "    print(f\"   Expected structure: {llm_traj_v2_dir}/v2_919_gpt4_8k_cot/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2ff74d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… TAU2-BENCH found at ../data/tau2_repo\n",
      "ğŸ”„ Processing TAU2 domain: airline\n",
      "  âœ… Inserted 50 airline tasks\n",
      "ğŸ”„ Processing TAU2 domain: retail\n",
      "  âœ… Inserted 114 retail tasks\n",
      "ğŸ”„ Processing TAU2 domain: telecom\n",
      "  âœ… Inserted 2285 telecom tasks\n",
      "ğŸ”„ Processing TAU2 domain: mock\n",
      "  âœ… Inserted 9 mock tasks\n",
      "ğŸ”„ Processing TAU2 results from ../data/tau2_repo/data/tau2/results/final\n",
      "âœ… Inserted 26 TAU2 result records\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: INGEST TAU2-BENCH DATASETS (FIXED MOCK DOMAIN)\n",
    "# ============================================================================\n",
    "\n",
    "# Assume tau2-bench has been cloned or downloaded manually\n",
    "tau2_repo_path = DATA_DIR / \"tau2_repo\"\n",
    "\n",
    "if not tau2_repo_path.exists():\n",
    "    print(f\"âš ï¸  TAU2-BENCH not found at {tau2_repo_path}\")\n",
    "    print(f\"   Please clone: git clone https://github.com/sierra-research/tau2-bench.git {tau2_repo_path}\")\n",
    "else:\n",
    "    print(f\"âœ… TAU2-BENCH found at {tau2_repo_path}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Parse TAU2 Domains (airline, retail, telecom, mock)\n",
    "    # =========================================================================\n",
    "    \n",
    "    tau2_domains_path = tau2_repo_path / \"data\" / \"tau2\" / \"domains\"\n",
    "    \n",
    "    if tau2_domains_path.exists():\n",
    "        # UPDATED SCHEMA: Changed task_num from INT to TEXT\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS tau2_domains (\n",
    "                task_id TEXT PRIMARY KEY,\n",
    "                domain TEXT,\n",
    "                task_num TEXT, \n",
    "                reason_for_call TEXT,\n",
    "                known_info TEXT,\n",
    "                unknown_info TEXT,\n",
    "                task_instructions TEXT,\n",
    "                actions_required TEXT,\n",
    "                source TEXT DEFAULT 'tau2'\n",
    "            )\n",
    "        \"\"\")\n",
    "        conn.commit()\n",
    "        \n",
    "        domains = ['airline', 'retail', 'telecom', 'mock']\n",
    "        \n",
    "        for domain in domains:\n",
    "            domain_path = tau2_domains_path / domain\n",
    "            tasks_file = domain_path / \"tasks.json\"\n",
    "            \n",
    "            if tasks_file.exists():\n",
    "                print(f\"ğŸ”„ Processing TAU2 domain: {domain}\")\n",
    "                \n",
    "                with open(tasks_file, 'r') as f:\n",
    "                    tasks = json.load(f)\n",
    "                \n",
    "                for task in tasks:\n",
    "                    task_id = f\"tau2_{domain}_{task['id']}\"\n",
    "                    user_scenario = task.get('user_scenario', {})\n",
    "                    \n",
    "                    # FIX: 'instructions' can be a string (in mock) or dict (others)\n",
    "                    instructions_raw = user_scenario.get('instructions', {})\n",
    "                    \n",
    "                    if isinstance(instructions_raw, dict):\n",
    "                        # Standard format (Airline, Retail, Telecom)\n",
    "                        instructions = instructions_raw\n",
    "                        task_instr = instructions.get('task_instructions', '')\n",
    "                    else:\n",
    "                        # Mock format: 'instructions' is just the instruction text directly\n",
    "                        instructions = {}\n",
    "                        task_instr = str(instructions_raw) # Treat the whole string as the task instruction\n",
    "                    \n",
    "                    # FIX: Treat ID as string, do not int()\n",
    "                    original_id = str(task['id'])\n",
    "                    eval_criteria = task.get('evaluation_criteria', {})\n",
    "                    \n",
    "                    cursor.execute(\"\"\"\n",
    "                        INSERT OR REPLACE INTO tau2_domains VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                    \"\"\", (\n",
    "                        task_id,\n",
    "                        domain,\n",
    "                        original_id,\n",
    "                        instructions.get('reason_for_call', ''),\n",
    "                        instructions.get('known_info', ''),\n",
    "                        instructions.get('unknown_info', ''),\n",
    "                        task_instr, # Uses the string directly if mock\n",
    "                        json.dumps(eval_criteria.get('actions', []))[:1000],\n",
    "                        'tau2'\n",
    "                    ))\n",
    "                \n",
    "                conn.commit()\n",
    "                print(f\"  âœ… Inserted {len(tasks)} {domain} tasks\")\n",
    "            else:\n",
    "                print(f\"  âš ï¸  {tasks_file} not found\")\n",
    "    else:\n",
    "        print(f\"âš ï¸  TAU2 domains path not found: {tau2_domains_path}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Parse TAU2 Results\n",
    "    # =========================================================================\n",
    "    \n",
    "    tau2_results_path = tau2_repo_path / \"data\" / \"tau2\" / \"results\" / \"final\"\n",
    "    \n",
    "    if tau2_results_path.exists():\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS tau2_results (\n",
    "                result_id TEXT PRIMARY KEY,\n",
    "                model TEXT,\n",
    "                domain TEXT,\n",
    "                num_trials INT,\n",
    "                result_json TEXT,\n",
    "                source TEXT DEFAULT 'tau2_results'\n",
    "            )\n",
    "        \"\"\")\n",
    "        conn.commit()\n",
    "        \n",
    "        print(f\"ğŸ”„ Processing TAU2 results from {tau2_results_path}\")\n",
    "        \n",
    "        for result_file in tau2_results_path.glob(\"*.json\"):\n",
    "            try:\n",
    "                with open(result_file, 'r') as f:\n",
    "                    result_data = json.load(f)\n",
    "                \n",
    "                result_id = result_file.stem\n",
    "                parts = result_file.stem.split('_')\n",
    "                model = parts[0] if parts else 'unknown'\n",
    "                domain = parts[1] if len(parts) > 1 else 'unknown'\n",
    "                \n",
    "                cursor.execute(\"\"\"\n",
    "                    INSERT OR REPLACE INTO tau2_results VALUES (?, ?, ?, ?, ?, ?)\n",
    "                \"\"\", (\n",
    "                    result_id,\n",
    "                    model,\n",
    "                    domain,\n",
    "                    len(result_data) if isinstance(result_data, list) else 1,\n",
    "                    json.dumps(result_data)[:5000],\n",
    "                    'tau2_results'\n",
    "                ))\n",
    "            except Exception as e:\n",
    "                print(f\"  âš ï¸  Error processing {result_file}: {e}\")\n",
    "        \n",
    "        conn.commit()\n",
    "        cursor.execute(\"SELECT COUNT(*) FROM tau2_results\")\n",
    "        count = cursor.fetchone()[0]\n",
    "        print(f\"âœ… Inserted {count} TAU2 result records\")\n",
    "    else:\n",
    "        print(f\"âš ï¸  TAU2 results path not found: {tau2_results_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "adb8d7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from database: ../data/db/unified.db\n",
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘              UNIFIED DATABASE SUMMARY                        â•‘\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "ğŸ“Š Data Ingestion Complete:\n",
      "   WebLINX samples:              1,000\n",
      "   WebArena tasks:               812\n",
      "   WebArena human traces:        2\n",
      "   WebArena LLM traces:          810\n",
      "   TAU2 domain tasks:            2,458\n",
      "   TAU2 result logs:             26\n",
      "\n",
      "   TOTAL RECORDS:                5,108\n",
      "\n",
      "âœ… Summary saved to ../data/processed/dataset_summary.json\n",
      "\n",
      "ğŸ“‹ Tables in database: ['weblinx', 'webarena_tasks', 'webarena_human_traces', 'webarena_llm_traces', 'tau2_domains', 'tau2_results']\n",
      "âœ… Database closed: ../data/db/unified.db\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 7: CREATE UNIFIED DATABASE VIEWS & SUMMARY (FIXED)\n",
    "# ============================================================================\n",
    "\n",
    "# Re-connect to ensure we are reading the actual persistent database file\n",
    "# This fixes \"Cannot operate on a closed database\" and ensures we see committed data\n",
    "if 'conn' in locals():\n",
    "    try:\n",
    "        conn.close()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "summary_stats = {}\n",
    "\n",
    "# Get counts for all tables\n",
    "tables = [\n",
    "    'weblinx',\n",
    "    'webarena_tasks',\n",
    "    'webarena_human_traces',\n",
    "    'webarena_llm_traces',\n",
    "    'tau2_domains',\n",
    "    'tau2_results'\n",
    "]\n",
    "\n",
    "print(f\"Reading from database: {DB_PATH}\")\n",
    "\n",
    "for table in tables:\n",
    "    try:\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
    "        count = cursor.fetchone()[0]\n",
    "        summary_stats[f'{table}_count'] = count\n",
    "    except sqlite3.OperationalError:\n",
    "        # Table doesn't exist\n",
    "        summary_stats[f'{table}_count'] = 0\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error reading {table}: {e}\")\n",
    "        summary_stats[f'{table}_count'] = 0\n",
    "\n",
    "# Print summary\n",
    "print(\"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\")\n",
    "print(\"â•‘              UNIFIED DATABASE SUMMARY                        â•‘\")\n",
    "print(\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "print(f\"\\nğŸ“Š Data Ingestion Complete:\")\n",
    "print(f\"   WebLINX samples:              {summary_stats.get('weblinx_count', 0):,}\")\n",
    "print(f\"   WebArena tasks:               {summary_stats.get('webarena_tasks_count', 0):,}\")\n",
    "print(f\"   WebArena human traces:        {summary_stats.get('webarena_human_traces_count', 0):,}\")\n",
    "print(f\"   WebArena LLM traces:          {summary_stats.get('webarena_llm_traces_count', 0):,}\")\n",
    "print(f\"   TAU2 domain tasks:            {summary_stats.get('tau2_domains_count', 0):,}\")\n",
    "print(f\"   TAU2 result logs:             {summary_stats.get('tau2_results_count', 0):,}\")\n",
    "\n",
    "total_samples = sum(summary_stats.values())\n",
    "print(f\"\\n   TOTAL RECORDS:                {total_samples:,}\")\n",
    "\n",
    "# Save summary\n",
    "summary_path = PROCESSED_DIR / \"dataset_summary.json\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump({\n",
    "        **summary_stats,\n",
    "        \"ingestion_date\": datetime.now().isoformat(),\n",
    "        \"database_path\": str(DB_PATH),\n",
    "        \"tables\": tables\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… Summary saved to {summary_path}\")\n",
    "\n",
    "# List all tables in database\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "all_tables = cursor.fetchall()\n",
    "print(f\"\\nğŸ“‹ Tables in database: {[t[0] for t in all_tables]}\")\n",
    "\n",
    "# Close database\n",
    "conn.close()\n",
    "print(f\"âœ… Database closed: {DB_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cceffe54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing database queries...\n",
      "\n",
      "1ï¸âƒ£  WebLINX sample:\n",
      "                  id  demo_id  turn_id                           action                                                                                                                                                                                                                                                                                  action_history       utterances                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       candidates                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      clean_html      viewport   source\n",
      "0  weblinx_apfyesq_6  apfyesq        6  click(uid=\"67e2a5fb-8b1d-41a0\")  say(speaker=\"navigator\", utterance=\"Hi\")</s><s>[INST] say(speaker=\"instructor\", utterance=\"Open Encyclopedia website.\") say(speaker=\"navigator\", utterance=\"Yes, sure\") load(url=\"https://www.encyclopedia.com/\")</s><s>[INST] say(speaker=\"instructor\", utterance=\"Search for biotechnology\")  [00:05] Hello ;  (uid = 67e2a5fb-8b1d-41a0) [[tag]] input [[xpath]] /html/body/div[2...form/div[1]/input [[bbox]] x=419.6 y=461.0 width=477.6 height=89.6 [[attributes]] title='' value='' name='...What do you want to learn today?' \\n(uid = fedfb512-949e-42b3) [[tag]] input [[xpath]] /html/body/div[2...form/div[2]/input [[bbox]] x=915.6 y=461.0 width=185.6 height=89.6 [[attributes]] type='submit' value='Search...-form-submit form-submit' \\n(uid = c7fbc11c-0949-4ab2) [[tag]] form [[xpath]] /html/body/div[2...2]/div[3]/form [[bbox]] x=419.6 y=461.0 width=680 height=88 [[attributes]] method='get' data-web...clopedia.com/gsearch' [[children]] div div \\n(uid = 6c7fe1f1-f640-4dce) [[tag]] span [[xpath]] /html/body/header/div...div[2]/a/span [[text]] EXPLORE [[bbox]] x=1240.5 y=28.6 width=54.1 height=30 [[attributes]] class='text' data-webtasks...-main-menu-menu' \\n(uid = 0ffc6f0e-808a-4c2a) [[tag]] span [[xpath]] /html/body/div[5]/span [[text]] Ã— [[bbox]] x=1485.9 y=665.6 width=23.3 height=21.6 [[attributes]] class='adthrive-close...8a-4c2a' \\n(uid = 8d8afc84-5b97-477a) [[tag]] div [[xpath]] /html/body/div[...3]/form/div[1] [[text]]   [[bbox]] x=419.6 y=461.0 width=476 height=88 [[attributes]] data-webtasks-id='8...keys form-no-label' [[children]] span input \\n(uid = 1ea51e98-3fcd-4e30) [[tag]] h4 [[xpath]] /html/body/div[...div/div[1]/h4 [[text]] The Worldâ€™s #1 Online Encyclopedia [[bbox]] x=33 y=163 width=1453.2 height=43.2 [[attributes]] data-webtasks-id='1...cd-4e30' \\n(uid = 769785af-485e-4cf1) [[tag]] a [[xpath]] /html/body/header/div...2]/div[2]/a [[bbox]] x=1240.5 y=28.6 width=74.1 height=30 [[attributes]] id='rcLink' class='... false; toggleFlyout()' [[children]] span i \\n(uid = e7b7879f-45ae-48a5) [[tag]] i [[xpath]] /html/body/header/div...div[2]/a/i [[bbox]] x=1294.6 y=33.6 width=20 height=20 [[attributes]] class='fa ency-down...5ae-48a5' \\n(uid = bf33a062-fb67-44f0) [[tag]] a [[xpath]] /html/body/div[2...div[4]/p/a [[text]] Read more [[bbox]] x=567.0 y=641.0 width=69.3 height=16 [[attributes]] href='/about' data-...67-44f0'  (html(body(div class=\"container\"(div class=\"row\"(div class=\"col hdr-r justify-...flex align-items-center\"(div class=\"hdr-categories-container\"(a class=\"rc-link\" onclick=\"if (!window.__cfRLUn... false; toggleFlyout()\" data-webtasks-id=\"76978...85e-4cf1\"(span class=\"text\" data-webtasks-id=\"6c7fe1...640-4dce\"EXPLORE)(i class=\"fa ency-down\" data-webtasks-id=\"e7b787...5ae-48a5\"))(div class=\"rc-flyout\"))))) (div (div class=\"dialog-off-canvas-main-canvas\"(div class=\"homepage\"(div style=\"background-image: url('/sites...01_3.png');\" class=\"ency-loaded\"(div class=\"ency-loaded mask-hero\")(h4 data-webtasks-id=\"1ea51e...fcd-4e30\"The Worldâ€™s #1 Online Encyclopedia)(div class=\"clear-both hero\"(div class=\"ency-hero-search\"(form action=\"https://www.encyclopedia.com/gsearch\" method=\"get\" data-webtasks-id=\"c7fbc11c...49-4ab2\"(div class=\"js-form-item form-...-keys form-no-label\" data-webtasks-id=\"8d8afc8...7-477a\" (span class=\"field-preffix\" (input class=\"button js-form-submit form-submit\" type=\"submit\" value=\"\" ) (input title=\"\" class=\"searchbox form-search form-input\" placeholder=\"What do you want to learn today?\" type=\"search\" name=\"q\" value=\"\" size=\"15\" maxlength=\"128\" data-webtasks-id=\"67e2a5...d-41a0\" spellcheck=\"false\" (span class=\"field-suffix\" (i class=\"fa ency-close\")))(div class=\"form-actions js-form-wrapper form-wrapper\" (input class=\"button js-form-submit form-submit\" type=\"submit\" value=\"Search\" data-webtasks-id=\"fedfb512-...9e-42b3\")))(div class=\"clear-both hero footer-copy\"(a href=\"/about\" data-webtasks-id=\"bf33a0...67-44f0\"Read more) about our content and why so many people love it.))))))(div class=\"adthrive-ad adth...cls adthrive-sticky\" style=\"min-height: 90px;\" closable=\"true\"(div style=\"border: 0pt none;\")(span class=\"adthrive-close\" data-webtasks-id=\"0ffc6f0...8a-4c2a\"Ã—))))  746h x 1536w  weblinx\n",
      "\n",
      "2ï¸âƒ£  WebArena tasks sample:\n",
      "   task_id               sites  require_login           start_url                                          intent                                         intent_template        eval_types                             reference_answers    source\n",
      "0        0  ['shopping_admin']              1  __SHOPPING_ADMIN__  What is the top-1 best-selling product in 2022  What is the top-{{n}} best-selling product in {{year}}  ['string_match']  {\"exact_match\": \"Quest Lumaflex\\u2122 Band\"}  webarena\n",
      "\n",
      "3ï¸âƒ£  WebArena LLM traces sample:\n",
      "                         trajectory_id            model config  task_id intent  passed  num_steps                                                                                                                                                                                                           observations                                                                                                                                                                                                       actions                                                              urls                                                                                                                                                                                                      predictions               source\n",
      "0  webarena_llm_919_gpt4_8k_cot_task_0  919_gpt4_8k_cot               0   What       0          1  [\"Tab 0 (current): Dashboard / Magento Admin\\n\\n[1] RootWebArea 'Dashboard / Magento Admin' focused: True\\n\\t[1306] link 'Magento Admin Panel'\\n\\t\\t[1307] img 'Magento Admin Panel'\\n\\t[1218] menubar '' orientati\"]  [\"{'action_type': , 'coords': array([0., 0.], dtype=float32), 'element_role': 0, 'element_name': '', 'text': [], 'page_number': 0, 'url': '', 'nth': 0, 'pw_code': '', 'element_id': '', 'key_comb': '', '\"]  [\"URL: http://metis.lti.cs.cmu.edu:7780/admin/admin/dashboard/\"]  [\"Let's think step-by-step. The information on the top-selling products is under the \\\"Bestsellers\\\" tab, which is currently expanded. The top-1 best-selling product is listed as \\\"Sprite Stasis Ball 65 cm\"]  webarena_llm_traces\n",
      "\n",
      "4ï¸âƒ£  TAU2 retail tasks sample:\n",
      "         task_id  domain  task_num                                                                                                                                                                                                                                                                                                reason_for_call                              known_info                             unknown_info                                                                 task_instructions                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     actions_required source\n",
      "0  tau2_retail_0  retail         0  You received your order #W2378156 and wish to exchange the mechanical keyboard for a similar one but with clicky switches and the smart thermostat for one compatible with Google Home instead of Apple HomeKit. If there is no keyboard that is clicky, RGB backlight, full size, you'd go for no backlight.  You are Yusuf Rossi in zip code 19122.  You do not remember your email address.  You are detail-oriented and want to make sure everything is addressed in one go.  [{\"action_id\": \"0_0\", \"name\": \"find_user_id_by_name_zip\", \"arguments\": {\"first_name\": \"Yusuf\", \"last_name\": \"Rossi\", \"zip\": \"19122\"}, \"info\": null}, {\"action_id\": \"0_1\", \"name\": \"get_order_details\", \"arguments\": {\"order_id\": \"#W2378156\"}, \"info\": null}, {\"action_id\": \"0_2\", \"name\": \"get_product_details\", \"arguments\": {\"product_id\": \"1656367028\"}, \"info\": null}, {\"action_id\": \"0_3\", \"name\": \"get_product_details\", \"arguments\": {\"product_id\": \"4896585277\"}, \"info\": null}, {\"action_id\": \"0_4\", \"name\": \"exchange_delivered_order_items\", \"arguments\": {\"order_id\": \"#W2378156\", \"item_ids\": [\"1151293680\", \"4983901480\"], \"new_item_ids\": [\"7706410293\", \"7747408585\"], \"payment_method_id\": \"credit_card_9513926\"}, \"info\": null}]   tau2\n",
      "\n",
      "âœ… Utility functions ready for use\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 8: UTILITY FUNCTIONS FOR QUERYING & EXPORTING\n",
    "# ============================================================================\n",
    "\n",
    "def get_db_connection():\n",
    "    \"\"\"Get connection to unified database\"\"\"\n",
    "    return sqlite3.connect(DB_PATH)\n",
    "\n",
    "def query_weblinx(limit: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"Query WebLINX samples\"\"\"\n",
    "    conn = get_db_connection()\n",
    "    df = pd.read_sql_query(f\"SELECT * FROM weblinx LIMIT {limit}\", conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "def query_webarena_tasks(limit: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"Query WebArena tasks\"\"\"\n",
    "    conn = get_db_connection()\n",
    "    df = pd.read_sql_query(f\"SELECT * FROM webarena_tasks LIMIT {limit}\", conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "def query_webarena_llm(model: Optional[str] = None, limit: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"Query WebArena LLM trajectories\"\"\"\n",
    "    conn = get_db_connection()\n",
    "    if model:\n",
    "        query = f\"SELECT * FROM webarena_llm_traces WHERE model LIKE '%{model}%' LIMIT {limit}\"\n",
    "    else:\n",
    "        query = f\"SELECT * FROM webarena_llm_traces LIMIT {limit}\"\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "def query_tau2(domain: str, limit: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"Query TAU2 tasks by domain\"\"\"\n",
    "    conn = get_db_connection()\n",
    "    df = pd.read_sql_query(\n",
    "        f\"SELECT * FROM tau2_domains WHERE domain = '{domain}' LIMIT {limit}\",\n",
    "        conn\n",
    "    )\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "def export_to_json(table_name: str, output_path: Path) -> None:\n",
    "    \"\"\"Export entire table to JSON\"\"\"\n",
    "    conn = get_db_connection()\n",
    "    df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n",
    "    conn.close()\n",
    "    df.to_json(output_path, orient='records', indent=2)\n",
    "    print(f\"âœ… Exported {len(df)} records to {output_path}\")\n",
    "\n",
    "# Test queries\n",
    "print(\"Testing database queries...\\n\")\n",
    "\n",
    "print(\"1ï¸âƒ£  WebLINX sample:\")\n",
    "print(query_weblinx(limit=1).to_string())\n",
    "\n",
    "print(\"\\n2ï¸âƒ£  WebArena tasks sample:\")\n",
    "try:\n",
    "    print(query_webarena_tasks(limit=1).to_string())\n",
    "except Exception as e:\n",
    "    print(f\"   (No data: {e})\")\n",
    "\n",
    "print(\"\\n3ï¸âƒ£  WebArena LLM traces sample:\")\n",
    "try:\n",
    "    print(query_webarena_llm(limit=1).to_string())\n",
    "except Exception as e:\n",
    "    print(f\"   (No data: {e})\")\n",
    "\n",
    "print(\"\\n4ï¸âƒ£  TAU2 retail tasks sample:\")\n",
    "try:\n",
    "    print(query_tau2(domain='retail', limit=1).to_string())\n",
    "except Exception as e:\n",
    "    print(f\"   (No data: {e})\")\n",
    "\n",
    "print(\"\\nâœ… Utility functions ready for use\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4384c47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Checking downloaded file structure...\n",
      "\n",
      "âœ… WebLinX config\n",
      "âœ… WebArena config\n",
      "âœ… WebArena human traces\n",
      "âœ… WebArena LLM v2\n",
      "âœ… TAU2 repo\n",
      "\n",
      "ğŸ“Š File Counts:\n",
      "   Human trace directories: 2\n",
      "   LLM model directories: 1\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# BONUS: QUICK FILE VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"ğŸ“ Checking downloaded file structure...\\n\")\n",
    "\n",
    "checks = {\n",
    "    \"WebLinX config\": (DATA_DIR / \"weblinx\" / \"templates\").exists(),\n",
    "    \"WebArena config\": (DATA_DIR / \"webarena\" / \"config_files\" / \"test.raw.json\").exists(),\n",
    "    \"WebArena human traces\": (DATA_DIR / \"webarena\" / \"human_trajectories\").exists() and len(list((DATA_DIR / \"webarena\" / \"human_trajectories\").glob(\"*.trace\"))) > 0,\n",
    "    \"WebArena LLM v2\": (DATA_DIR / \"webarena\" / \"llm_trajectories_v2\").exists() and len(list((DATA_DIR / \"webarena\" / \"llm_trajectories_v2\").glob(\"*/\"))) > 0,\n",
    "    \"TAU2 repo\": (DATA_DIR / \"tau2_repo\" / \"data\" / \"tau2\" / \"domains\").exists(),\n",
    "}\n",
    "\n",
    "for check_name, exists in checks.items():\n",
    "    status = \"âœ…\" if exists else \"âš ï¸ \"\n",
    "    print(f\"{status} {check_name}\")\n",
    "\n",
    "# Count files\n",
    "human_traces = list((DATA_DIR / \"webarena\" / \"human_trajectories\").glob(\"*.trace\"))\n",
    "llm_models = list((DATA_DIR / \"webarena\" / \"llm_trajectories_v2\").glob(\"*/\"))\n",
    "\n",
    "print(f\"\\nğŸ“Š File Counts:\")\n",
    "print(f\"   Human trace directories: {len(human_traces)}\")\n",
    "print(f\"   LLM model directories: {len(llm_models)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fca40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HARMONIZATION: EXPORT TO TRAINING FORMAT\n",
    "# ============================================================================\n",
    "\n",
    "TRAINING_DATA_PATH = PROCESSED_DIR / \"agent_training_data_unified.json\"\n",
    "\n",
    "def normalize_weblinx(row):\n",
    "    \"\"\"Convert WebLINX row to standard format\"\"\"\n",
    "    return {\n",
    "        \"id\": row['id'],\n",
    "        \"source\": \"weblinx\",\n",
    "        \"prompt\": row['utterances'], # You might need to parse this to get just the instruction\n",
    "        \"trace\": row['action_history'], # Already sequential\n",
    "        \"label\": 0 # Assume benign for now\n",
    "    }\n",
    "\n",
    "def normalize_tau2(row):\n",
    "    \"\"\"Convert TAU2 row to standard format\"\"\"\n",
    "    return {\n",
    "        \"id\": row['task_id'],\n",
    "        \"source\": \"tau2\",\n",
    "        \"prompt\": row['task_instructions'],\n",
    "        \"trace\": row['actions_required'], # These are gold-standard actions\n",
    "        \"label\": 0 # Assume benign (gold standard)\n",
    "    }\n",
    "\n",
    "def normalize_webarena_llm(row):\n",
    "    \"\"\"Convert WebArena LLM Trace to standard format\"\"\"\n",
    "    # row keys: trajectory_id, intent, actions, passed\n",
    "    return {\n",
    "        \"id\": row['trajectory_id'],\n",
    "        \"source\": \"webarena_llm\",\n",
    "        \"prompt\": row['intent'],\n",
    "        \"trace\": row['actions'],\n",
    "        \"label\": 1 if row['passed'] == 0 else 0 # FAIL might imply vulnerability or just incompetence. \n",
    "        # For adversarial training, you might care specifically about *successful* attacks. \n",
    "        # Adjust logic based on whether you want 'failed tasks' or 'successful attacks'.\n",
    "    }\n",
    "\n",
    "# --- EXECUTE EXPORT ---\n",
    "\n",
    "all_training_data = []\n",
    "\n",
    "# 1. Export WebLINX\n",
    "df_weblinx = query_weblinx(limit=1000)\n",
    "all_training_data.extend([normalize_weblinx(r) for _, r in df_weblinx.iterrows()])\n",
    "\n",
    "# 2. Export TAU2\n",
    "df_tau2 = query_tau2('retail', limit=1000) # Do for all domains\n",
    "all_training_data.extend([normalize_tau2(r) for _, r in df_tau2.iterrows()])\n",
    "\n",
    "# 3. Export WebArena\n",
    "df_wa = query_webarena_llm(limit=1000)\n",
    "all_training_data.extend([normalize_webarena_llm(r) for _, r in df_wa.iterrows()])\n",
    "\n",
    "# Save\n",
    "with open(TRAINING_DATA_PATH, 'w') as f:\n",
    "    json.dump(all_training_data, f, indent=2)\n",
    "\n",
    "print(f\"ğŸš€ Harmonized {len(all_training_data)} samples into {TRAINING_DATA_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent_adversarial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
